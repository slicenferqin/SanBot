#!/usr/bin/env python3
"""
æŠ–éŸ³æ•°æ®æŠ“å–å·¥å…· V2 - å¢å¼ºç‰ˆ
æ”¯æŒå¤šç§æ–¹å¼è·å–ç”¨æˆ·æ•°æ®
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright
from datetime import datetime


class DouyinScraperV2:
    def __init__(self, headless=False):
        self.headless = headless
        self.base_url = "https://www.douyin.com"
        
    async def scrape_by_direct_url(self, user_id, sec_user_id=None):
        """
        é€šè¿‡ç›´æ¥URLè®¿é—®ç”¨æˆ·ä¸»é¡µ
        user_id: æ•°å­—ID
        sec_user_id: åŠ å¯†çš„ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                ]
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
                viewport={'width': 390, 'height': 844}  # iPhone å°ºå¯¸
            )
            
            page = await context.new_page()
            
            try:
                # æ„å»ºç”¨æˆ·ä¸»é¡µURL
                if sec_user_id:
                    user_url = f"{self.base_url}/user/{sec_user_id}"
                else:
                    user_url = f"{self.base_url}/user/{user_id}"
                
                print(f"ğŸš€ ç›´æ¥è®¿é—®ç”¨æˆ·ä¸»é¡µ: {user_url}")
                
                await page.goto(user_url, wait_until='networkidle', timeout=30000)
                await asyncio.sleep(3)
                
                # æˆªå›¾
                screenshot_path = f"/tmp/douyin_direct_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                await page.screenshot(path=screenshot_path, full_page=True)
                print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                
                # æå–æ•°æ®
                data = await self._extract_all_data(page)
                data['url'] = user_url
                data['screenshot'] = screenshot_path
                
                return data
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return None
            finally:
                await browser.close()
    
    async def search_and_extract(self, keyword):
        """
        æœç´¢å¹¶æå–ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
                viewport={'width': 390, 'height': 844}
            )
            
            page = await context.new_page()
            
            try:
                # ä½¿ç”¨ç§»åŠ¨ç«¯æœç´¢
                search_url = f"{self.base_url}/search/{keyword}"
                print(f"ğŸ” æœç´¢URL: {search_url}")
                
                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
                
                # æˆªå›¾
                await page.screenshot(path="/tmp/douyin_search_v2.png", full_page=True)
                print("ğŸ“¸ æœç´¢ç»“æœå·²æˆªå›¾")
                
                # å°è¯•æ‰¾åˆ°ç”¨æˆ·é“¾æ¥
                user_links = await page.evaluate('''() => {
                    const links = [];
                    const allLinks = document.querySelectorAll('a');
                    allLinks.forEach(link => {
                        const href = link.getAttribute('href');
                        const text = link.textContent.trim();
                        if (href && (href.includes('/user/') || text.includes('è´¾ä¹ƒäº®'))) {
                            links.push({
                                href: href,
                                text: text.substring(0, 50)
                            });
                        }
                    });
                    return links;
                }''')
                
                print(f"ğŸ”— æ‰¾åˆ° {len(user_links)} ä¸ªç›¸å…³é“¾æ¥:")
                for i, link in enumerate(user_links[:10]):
                    print(f"  [{i+1}] {link['text']} -> {link['href']}")
                
                # ä¿å­˜ç»“æœ
                result = {
                    "search_keyword": keyword,
                    "found_links": user_links,
                    "screenshot": "/tmp/douyin_search_v2.png",
                    "timestamp": datetime.now().isoformat()
                }
                
                return result
                
            except Exception as e:
                print(f"âŒ æœç´¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return None
            finally:
                await browser.close()
    
    async def _extract_all_data(self, page):
        """
        æå–æ‰€æœ‰å¯ç”¨æ•°æ®
        """
        data = {
            "timestamp": datetime.now().isoformat(),
            "user_info": {},
            "stats": {},
            "recent_videos": []
        }
        
        try:
            # 1. å°è¯•ä» script æ ‡ç­¾æå–æ¸²æŸ“æ•°æ®
            print("ğŸ” æå–æ¸²æŸ“æ•°æ®...")
            render_data = await page.evaluate('''() => {
                // æŸ¥æ‰¾åŒ…å«æ•°æ®çš„ script æ ‡ç­¾
                const scripts = Array.from(document.querySelectorAll('script'));
                const dataScripts = scripts.filter(s => 
                    s.textContent.includes('__RENDER_DATA__') ||
                    s.textContent.includes('SSR_HYDRATED_DATA')
                );
                
                if (dataScripts.length > 0) {
                    return dataScripts[0].textContent.substring(0, 50000);
                }
                return null;
            }''')
            
            if render_data:
                # ä¿å­˜åŸå§‹æ•°æ®
                data['raw_script'] = render_data
                
                # å°è¯•è§£æJSON
                patterns = [
                    r'__RENDER_DATA__\s*=\s*({.*?});\s*<\/script>',
                    r'_SSR_HYDRATED_DATA\s*=\s*({.*?});',
                    r'"data":\s*({.*?})\s*,"env"',
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, render_data, re.DOTALL)
                    if match:
                        try:
                            json_str = match.group(1)
                            json_data = json.loads(json_str)
                            data['parsed_data'] = json_data
                            print("âœ… æˆåŠŸè§£æJSONæ•°æ®!")
                            break
                        except:
                            continue
            
            # 2. ä»é¡µé¢å…ƒç´ æå–å¯è§æ•°æ®
            print("ğŸ” æå–å¯è§æ•°æ®...")
            visible_data = await page.evaluate('''() => {
                const result = {
                    title: document.title,
                    url: window.location.href
                };
                
                // å°è¯•å¤šç§é€‰æ‹©å™¨
                const selectors = {
                    // æŠ–éŸ³å¸¸è§çš„æ•°æ®é€‰æ‹©å™¨
                    'user-post-count': '[data-e2e="user-post-count"]',
                    'user-following-count': '[data-e2e="user-following-count"]',
                    'user-follower-count': '[data-e2e="user-follower-count"]',
                    'user-like-count': '[data-e2e="user-like-count"]',
                    // é€šç”¨é€‰æ‹©å™¨
                    'follower': '.follower-count, .fans-count, [class*="follower"], [class*="fans"]',
                    'following': '.following-count, [class*="following"]',
                    'works': '.works-count, .post-count, [class*="works"], [class*="post"]',
                    'likes': '.like-count, [class*="like"]',
                };
                
                for (key, selector) of Object.entries(selectors)) {
                    const elements = document.querySelectorAll(selector);
                    if (elements.length > 0) {
                        result[key] = Array.from(elements).map(el => el.textContent.trim());
                    }
                }
                
                // è·å–é¡µé¢å‰5000ä¸ªå­—ç¬¦ç”¨äºåˆ†æ
                result.body_preview = document.body.textContent.substring(0, 5000);
                
                return result;
            }''')
            
            data['visible_data'] = visible_data
            
            # 3. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»é¡µé¢æºç ä¸­æå–æ•°å­—
            print("ğŸ” ä½¿ç”¨æ­£åˆ™æå–æ•°æ®...")
            page_content = await page.content()
            
            # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®æ¨¡å¼
            patterns = {
                'followers': r'ç²‰ä¸[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒä¸‡äº¿]?)',
                'following': r'å…³æ³¨[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒä¸‡äº¿]?)',
                'likes': r'è·èµ[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒä¸‡äº¿]?)',
                'works': r'ä½œå“[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒä¸‡äº¿]?)',
            }
            
            extracted_stats = {}
            for key, pattern in patterns.items():
                matches = re.findall(pattern, page_content)
                if matches:
                    extracted_stats[key] = matches[:5]  # åªä¿ç•™å‰5ä¸ªåŒ¹é…
            
            data['regex_extracted'] = extracted_stats
            
            return data
            
        except Exception as e:
            print(f"âš ï¸  æ•°æ®æå–å¤±è´¥: {e}")
            data['error'] = str(e)
            return data


async def main():
    """
    ä¸»å‡½æ•° - å°è¯•å¤šç§æ–¹æ³•
    """
    print("=" * 70)
    print("ğŸ¬ æŠ–éŸ³æ•°æ®æŠ“å–å·¥å…· V2 - å¢å¼ºç‰ˆ")
    print("=" * 70)
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    scraper = DouyinScraperV2(headless=False)
    
    # æ–¹æ³•1: æœç´¢
    print("\nã€æ–¹æ³•1ã€‘æœç´¢è´¾ä¹ƒäº®")
    print("-" * 70)
    search_result = await scraper.search_and_extract("è´¾ä¹ƒäº®")
    
    if search_result:
        print("\nğŸ“Š æœç´¢ç»“æœ:")
        print(json.dumps(search_result, indent=2, ensure_ascii=False)[:1000])
        
        output_file = f"/tmp/douyin_search_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(search_result, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å·²ä¿å­˜: {output_file}")
    
    # æ–¹æ³•2: å°è¯•å·²çŸ¥çš„è´¾ä¹ƒäº®è´¦å·IDï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # æ³¨æ„ï¼šè¿™äº›IDéœ€è¦ä»å®é™…æ•°æ®ä¸­è·å–
    print("\nã€æ–¹æ³•2ã€‘ç›´æ¥è®¿é—®ï¼ˆéœ€è¦æ­£ç¡®çš„ç”¨æˆ·IDï¼‰")
    print("-" * 70)
    print("âš ï¸  éœ€è¦æä¾›æ­£ç¡®çš„ç”¨æˆ·ID (MS4wLjABAAAA...) æˆ–æ•°å­—ID")
    print("ğŸ’¡ æç¤ºï¼šå¯ä»¥ä»æœç´¢ç»“æœæˆ–åˆ†äº«é“¾æ¥ä¸­è·å–")
    
    print("\n" + "=" * 70)
    print("âœ… æŠ“å–å®Œæˆ!")
    print("=" * 70)
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - /tmp/douyin_search_v2.png (æœç´¢ç»“æœæˆªå›¾)")
    print("  - /tmp/douyin_search_result_*.json (æœç´¢ç»“æœæ•°æ®)")
    print("\nğŸ’¡ å»ºè®®:")
    print("  1. æŸ¥çœ‹æˆªå›¾æ‰¾åˆ°æ­£ç¡®çš„ç”¨æˆ·é“¾æ¥")
    print("  2. ä»åˆ†äº«é“¾æ¥ä¸­æå– sec_user_id")
    print("  3. ä½¿ç”¨æ­£ç¡®çš„IDå†æ¬¡è¿è¡Œç›´æ¥è®¿é—®")


if __name__ == "__main__":
    asyncio.run(main())
