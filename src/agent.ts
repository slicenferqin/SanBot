import Anthropic from '@anthropic-ai/sdk';
import { generateText, jsonSchema, streamText, stepCountIs, type ModelMessage } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import type { Config } from './config/types.ts';
import { createToolRegistry, createToolRegistryWithDynamic, type ToolRegistry } from './tools/index.ts';
import {
  saveConversation,
  getSessionContext,
  formatMemoryContext,
  appendSessionSummary,
  appendExtractedMemory,
  type ToolCallRecord,
  type ConversationRecord,
} from './memory/index.ts';
import { loadSoul } from './birth/index.ts';
import { ToolSpinner, StreamWriter } from './tui/index.ts';
import type { ToolSpinnerInterface, StreamWriterInterface } from './tui/index.ts';
import {
  gatherRuntimeContext,
  formatRuntimeContext,
  ContextCompactor,
  type CompactionConfig,
} from './context/index.ts';
import { SubagentRunner, type SubagentTask, type SubagentResult } from './agent/subagent.ts';
import {
  MCPManager,
  loadMCPConfig,
  getMCPToolDefs,
  type MCPServerConfig,
} from './mcp/index.ts';
import { pc } from './tui-v3/utils.ts';

type CoreMessage = {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content: string;
};

/**
 * Agent é…ç½®
 */
export interface AgentConfig {
  llmConfig: Config['llm'];
  maxSteps?: number;
  sessionId?: string;
  compaction?: Partial<CompactionConfig>;
  /** æ˜¯å¦å¯ç”¨ MCP */
  enableMCP?: boolean;
}

/**
 * Agent æ ¸å¿ƒç±» - æ”¯æŒå¤šæœåŠ¡å•†
 */
export class Agent {
  private toolRegistry: ToolRegistry;
  private config: AgentConfig;
  private sessionId: string;
  // å¯¹è¯å†å² - æ”¯æŒå¤šè½®å¯¹è¯
  private conversationHistory: Anthropic.MessageParam[] = [];
  private openaiHistory: ModelMessage[] = [];
  // å½“å‰å¯¹è¯çš„å·¥å…·è°ƒç”¨è®°å½•
  private currentToolCalls: ToolCallRecord[] = [];
  // è®°å¿†ä¸Šä¸‹æ–‡
  private memoryContext: string = '';
  private runtimeContext: string = '';
  private shortTermSummary: string = '';
  // çµé­‚è®°å½•
  private soulContext: string = '';
  // æ˜¯å¦å·²åˆå§‹åŒ–
  private initialized: boolean = false;
  // ä¸Šä¸‹æ–‡å‹ç¼©å™¨
  private compactor: ContextCompactor;
  // MCP ç®¡ç†å™¨
  private mcpManager: MCPManager | null = null;

  constructor(config: AgentConfig) {
    this.config = config;
    this.sessionId =
      config.sessionId || `${Date.now()}-${Math.random().toString(36).slice(2, 8)}`;
    // å…ˆç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œinit æ—¶ä¼šæ›¿æ¢
    this.toolRegistry = createToolRegistry();
    // åˆå§‹åŒ–ä¸Šä¸‹æ–‡å‹ç¼©å™¨
    this.compactor = new ContextCompactor(config.llmConfig, config.compaction);
  }

  /**
   * åˆå§‹åŒ– Agentï¼ˆåŠ è½½è®°å¿†å’Œè‡ªåˆ›å»ºå·¥å…·ï¼‰
   */
  async init(): Promise<void> {
    if (this.initialized) return;

    // å¹¶è¡ŒåŠ è½½è®°å¿†ã€çµé­‚ã€è‡ªåˆ›å»ºå·¥å…·
    const [context, soul, registry, runtime] = await Promise.all([
      getSessionContext(),
      loadSoul(),
      createToolRegistryWithDynamic(),
      gatherRuntimeContext(process.cwd()).catch(() => null),
    ]);

    this.memoryContext = formatMemoryContext(context);
    this.soulContext = soul || '';
    this.toolRegistry = registry;
    if (runtime) {
      this.runtimeContext = formatRuntimeContext(runtime);
    }

    // åˆå§‹åŒ– MCPï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if (this.config.enableMCP) {
      await this.initMCP();
    }

    this.initialized = true;
  }

  /**
   * åˆå§‹åŒ– MCP è¿æ¥
   */
  private async initMCP(): Promise<void> {
    try {
      const mcpConfig = await loadMCPConfig();
      if (mcpConfig.servers.length === 0) {
        return;
      }

      this.mcpManager = new MCPManager();
      for (const server of mcpConfig.servers) {
        this.mcpManager.addServer(server);
      }

      await this.mcpManager.connectAll();

      // å°† MCP å·¥å…·æ³¨å†Œåˆ°å·¥å…·æ³¨å†Œè¡¨
      for (const { server, tool } of this.mcpManager.getAllTools()) {
        const client = this.mcpManager.getClient(server);
        if (client) {
          const toolDefs = getMCPToolDefs(client);
          for (const toolDef of toolDefs) {
            this.toolRegistry.register(toolDef);
          }
        }
      }

      console.log(pc.gray(`[Agent] MCP initialized with ${this.mcpManager.getAllTools().length} tools`));
    } catch (error: any) {
      console.warn(pc.yellow(`[Agent] MCP initialization failed: ${error.message}`));
    }
  }

  /**
   * åˆå§‹åŒ–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
   * @deprecated ä½¿ç”¨ init() ä»£æ›¿
   */
  async initMemory(): Promise<void> {
    await this.init();
  }

  /**
   * çƒ­æ›´æ–° LLM é…ç½®
   */
  updateLLMConfig(llmConfig: Config['llm']): void {
    this.config.llmConfig = llmConfig;
    // æ¸…ç©ºå¯¹è¯å†å²ï¼Œå› ä¸ºä¸åŒæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ ¼å¼å¯èƒ½ä¸åŒ
    this.conversationHistory = [];
    this.openaiHistory = [];
    // æ›´æ–° compactor çš„ LLM é…ç½®
    this.compactor = new ContextCompactor(llmConfig, this.config.compaction);
    console.log(pc.gray(`[Agent] LLM config updated: ${llmConfig.provider} / ${llmConfig.model}`));
  }

  /**
   * è·å–å½“å‰é…ç½®
   */
  getConfig(): AgentConfig {
    return { ...this.config };
  }

  /**
   * è·å–å½“å‰ä¼šè¯ ID
   */
  getSessionId(): string {
    return this.sessionId;
  }

  /**
   * æ¸…ç©ºå¯¹è¯å†å²
   */
  clearHistory(): void {
    this.conversationHistory = [];
    this.openaiHistory = [];
  }

  /**
   * ä»æŒä¹…åŒ–è®°å½•æ¢å¤ä¼šè¯å†å²ï¼ˆç”¨äºæœåŠ¡é‡å¯åçš„ä¸Šä¸‹æ–‡å»¶ç»­ï¼‰
   */
  hydrateConversationHistory(records: ConversationRecord[], maxTurns: number = 30): void {
    const safeMaxTurns = Number.isFinite(maxTurns) && maxTurns > 0
      ? Math.min(Math.floor(maxTurns), 100)
      : 30;

    const trimmedRecords = records
      .filter((record) => Boolean(record?.userMessage || record?.assistantResponse))
      .slice(-safeMaxTurns);

    const anthropicHistory: Anthropic.MessageParam[] = [];
    const openaiHistory: ModelMessage[] = [];

    for (const record of trimmedRecords) {
      const userMessage = record.userMessage?.trim();
      const assistantResponse = record.assistantResponse?.trim();

      if (userMessage) {
        anthropicHistory.push({ role: 'user', content: userMessage });
        openaiHistory.push({ role: 'user', content: userMessage });
      }

      if (assistantResponse) {
        anthropicHistory.push({ role: 'assistant', content: assistantResponse });
        openaiHistory.push({ role: 'assistant', content: assistantResponse });
      }
    }

    this.conversationHistory = anthropicHistory;
    this.openaiHistory = openaiHistory;

    console.log(pc.gray(`[Agent] Restored ${trimmedRecords.length} turns for session ${this.sessionId}`));
  }

  /**
   * å§”æ´¾ä»»åŠ¡ç»™å­ä»£ç†æ‰§è¡Œ
   * å­ä»£ç†åœ¨ç‹¬ç«‹ä¸Šä¸‹æ–‡ä¸­å·¥ä½œï¼Œåªå›ä¼ ç»“æ„åŒ–æ‘˜è¦
   */
  async delegateToSubagent(task: SubagentTask): Promise<SubagentResult> {
    const runner = new SubagentRunner(this.config.llmConfig);
    return runner.run(task);
  }

  /**
   * å¹¶è¡Œå§”æ´¾å¤šä¸ªä»»åŠ¡ç»™å­ä»£ç†
   */
  async delegateParallel(tasks: SubagentTask[]): Promise<SubagentResult[]> {
    const runner = new SubagentRunner(this.config.llmConfig);
    return runner.runParallel(tasks);
  }

  /**
   * å°†å­ä»£ç†ç»“æœåˆå¹¶ä¸ºå¯æ³¨å…¥ä¸Šä¸‹æ–‡çš„æ‘˜è¦
   */
  mergeSubagentResults(results: SubagentResult[]): string {
    return SubagentRunner.mergeResults(results);
  }

  /**
   * æ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²ï¼ˆä½¿ç”¨ Compaction ç­–ç•¥ï¼‰
   * ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼Œé¿å… token è¶…é™
   */
  private async trimConversationHistory(): Promise<void> {
    // ä½¿ç”¨ compactor æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    if (this.compactor.shouldCompact(this.conversationHistory)) {
      const result = await this.compactor.compact(
        this.conversationHistory,
        this.sessionId
      );

      if (result.compacted) {
        // ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
        const keepCount = 20; // ä¸ compaction config ä¿æŒä¸€è‡´
        this.conversationHistory = this.conversationHistory.slice(-keepCount);

        // æ›´æ–°çŸ­æœŸæ‘˜è¦
        if (result.summary) {
          this.shortTermSummary = this.mergeShortSummary(
            result.summary,
            this.shortTermSummary
          );
        }
      }
    }

    // åŒæ ·å¤„ç† OpenAI history
    if (this.compactor.shouldCompact(this.openaiHistory as any)) {
      const result = await this.compactor.compact(
        this.openaiHistory as any,
        this.sessionId
      );

      if (result.compacted) {
        const keepCount = 20;
        this.openaiHistory = this.openaiHistory.slice(-keepCount);

        if (result.summary && !this.shortTermSummary.includes(result.summary)) {
          this.shortTermSummary = this.mergeShortSummary(
            result.summary,
            this.shortTermSummary
          );
        }
      }
    }
  }

  private handleHistoryRemoval(messages: Array<{ role: string; content: any }>): void {
    if (!messages.length) return;
    const fragment = this.createHistoryFragment(messages);
    if (!fragment) return;
    this.shortTermSummary = this.mergeShortSummary(fragment, this.shortTermSummary);
    appendSessionSummary(this.sessionId, fragment).catch((error) => {
      console.warn('Unable to persist session summary', error);
    });
    appendExtractedMemory('runtime', fragment).catch((error) => {
      console.warn('Unable to persist extracted memory', error);
    });
  }

  private createHistoryFragment(messages: Array<{ role: string; content: any }>): string {
    const snapshots = messages
      .map((msg) => {
        const text = this.normalizeMessageContent(msg.content);
        if (!text) return '';
        return `[${msg.role}] ${text}`;
      })
      .filter(Boolean);
    return snapshots.join(' | ');
  }

  private mergeShortSummary(fragment: string, existing: string): string {
    if (!fragment) return existing;
    const combined = existing ? `${existing}\n${fragment}` : fragment;
    const maxChars = 2000;
    return combined.length > maxChars ? combined.slice(combined.length - maxChars) : combined;
  }

  private normalizeMessageContent(content: any): string {
    if (!content) return '';
    if (typeof content === 'string') {
      return content.slice(0, 240);
    }
    if (Array.isArray(content)) {
      return content
        .map((block) => {
          if (typeof block === 'string') return block;
          if (block && typeof block === 'object' && 'text' in block) {
            return String(block.text);
          }
          if (block && typeof block === 'object' && 'type' in block) {
            return `[${block.type}]`;
          }
          return '';
        })
        .filter(Boolean)
        .join(' ')
        .slice(0, 240);
    }
    if (content && typeof content === 'object' && 'content' in content) {
      return this.normalizeMessageContent((content as any).content);
    }
    try {
      return JSON.stringify(content).slice(0, 240);
    } catch {
      return '';
    }
  }

  /**
   * ç”Ÿæˆä¸»åŠ¨é—®å€™è¯­
   * åŸºäºç”¨æˆ·ç”»åƒã€è®°å¿†å’Œå½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡
   */
  async generateGreeting(projectContext?: string): Promise<string> {
    const { provider } = this.config.llmConfig;

    const greetingPrompt = `Generate a warm, personalized greeting for the user based on the following context:

${this.memoryContext ? `## User Memory Context\n${this.memoryContext}\n` : ''}
${projectContext ? `## Current Project Context\n${projectContext}\n` : ''}

Guidelines:
1. Keep it concise (2-4 sentences)
2. Reference specific details from their profile or recent activities if available
3. Ask a relevant question about their current work or interests
4. Be friendly but professional
5. Use the user's name if known from the profile

Generate only the greeting text, no additional formatting.`;

    let greeting: string;

    if (provider === 'anthropic') {
      const client = new Anthropic({
        apiKey: this.config.llmConfig.apiKey,
        baseURL: this.config.llmConfig.baseUrl,
      });

      const response = await client.messages.create({
        model: this.config.llmConfig.model,
        max_tokens: 256,
        system: this.getSystemPrompt(),
        messages: [{ role: 'user', content: greetingPrompt }],
        temperature: this.getTemperature(),
      });

      greeting = response.content
        .filter((block): block is Anthropic.TextBlock => block.type === 'text')
        .map((b) => b.text)
        .join('\n');
    } else {
      const openaiOptions: Record<string, unknown> = {
        apiKey: this.config.llmConfig.apiKey,
        baseURL: this.config.llmConfig.baseUrl,
        headers: this.config.llmConfig.headers,
      };
      if (this.config.llmConfig.provider === 'openai-compatible') {
        openaiOptions.compatibility = 'compatible';
      }
      const openai = createOpenAI(openaiOptions as any);

      const model = this.config.llmConfig.api === 'responses'
        ? openai.responses(this.config.llmConfig.model)
        : openai.chat(this.config.llmConfig.model);

      const result = await generateText({
        model,
        messages: [{ role: 'user', content: greetingPrompt }],
        system: this.getSystemPrompt(),
        temperature: this.getTemperature(),
      });

      greeting = result.text;
    }

    return greeting.trim();
  }

  /**
   * æ‰§è¡Œå¯¹è¯ï¼ˆæ”¯æŒå¤šè½®ä¸Šä¸‹æ–‡ï¼‰
   */
  async chat(userMessage: string): Promise<string> {
    // é‡ç½®å½“å‰å·¥å…·è°ƒç”¨è®°å½•
    this.currentToolCalls = [];

    const { provider } = this.config.llmConfig;

    let response: string;

    // Anthropic ä½¿ç”¨åŸç”Ÿ SDKï¼ˆæ›´å¥½çš„å…¼å®¹æ€§ï¼‰
    if (provider === 'anthropic') {
      response = await this.chatWithAnthropic(userMessage);
    } else {
      // OpenAI å…¼å®¹æœåŠ¡å•†ä½¿ç”¨ AI SDK
      response = await this.chatWithOpenAI(userMessage);
    }

    // ä¿å­˜å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ
    await saveConversation(
      this.sessionId,
      userMessage,
      response,
      this.currentToolCalls.length > 0 ? this.currentToolCalls : undefined
    );

    return response;
  }

  /**
   * æµå¼å¯¹è¯ï¼ˆå®æ—¶æ˜¾ç¤ºå“åº”ï¼‰
   */
  async chatStream(
    userMessage: string,
    streamWriter?: StreamWriterInterface,
    toolSpinner?: ToolSpinnerInterface
  ): Promise<string> {
    // é‡ç½®å½“å‰å·¥å…·è°ƒç”¨è®°å½•
    this.currentToolCalls = [];

    const { provider } = this.config.llmConfig;

    let response: string;

    // Anthropic ä½¿ç”¨åŸç”Ÿ SDKï¼ˆæ›´å¥½çš„å…¼å®¹æ€§ï¼‰
    if (provider === 'anthropic') {
      response = await this.chatWithAnthropicStream(userMessage, streamWriter, toolSpinner);
    } else {
      // OpenAI å…¼å®¹æœåŠ¡å•†ä½¿ç”¨ AI SDK
      response = await this.chatWithOpenAIStream(userMessage, streamWriter, toolSpinner);
    }

    // ä¿å­˜å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ
    await saveConversation(
      this.sessionId,
      userMessage,
      response,
      this.currentToolCalls.length > 0 ? this.currentToolCalls : undefined
    );

    return response;
  }

  /**
   * ä½¿ç”¨ Anthropic SDK å¯¹è¯
   */
  private async chatWithAnthropic(userMessage: string): Promise<string> {
    const client = new Anthropic({
      apiKey: this.config.llmConfig.apiKey,
      baseURL: this.config.llmConfig.baseUrl,
    });

    const tools = this.toolRegistry.getAll().map((t) => ({
      name: t.name,
      description: t.description,
      input_schema: t.schema as Anthropic.Tool['input_schema'],
    }));

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    this.conversationHistory.push({ role: 'user', content: userMessage });

    let response = await client.messages.create({
      model: this.config.llmConfig.model,
      max_tokens: 4096,
      system: this.getSystemPrompt(),
      messages: this.conversationHistory,
      tools,
      temperature: this.getTemperature(),
    });

    // å¤„ç†å·¥å…·è°ƒç”¨å¾ªç¯
    let steps = 0;
    const maxSteps = this.config.maxSteps || 999;

    while (response.stop_reason === 'tool_use' && steps < maxSteps) {
      steps++;

      // é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ content å­˜åœ¨ä¸”æ˜¯æ•°ç»„
      if (!response.content || !Array.isArray(response.content)) {
        console.error('âŒ Error: response.content is not an array:', response.content);
        break;
      }

      const toolUseBlocks = response.content.filter(
        (block): block is Anthropic.ToolUseBlock => block.type === 'tool_use'
      );

      this.conversationHistory.push({ role: 'assistant', content: response.content });

      const toolResults: Anthropic.ToolResultBlockParam[] = [];

      for (const toolUse of toolUseBlocks) {
        console.log(`\nğŸ”§ Calling tool: ${toolUse.name}`);
        console.log(`   Input: ${JSON.stringify(toolUse.input)}`);

        const tool = this.toolRegistry.get(toolUse.name);
        if (tool) {
          let result: any;
          try {
            result = await tool.execute(toolUse.input);
          } catch (error: any) {
            result = {
              success: false,
              error: error?.message || 'Tool execution failed',
            };
          }
          console.log(`   Result: ${result.success ? 'âœ…' : 'âŒ'}`);

          // è®°å½•å·¥å…·è°ƒç”¨
          this.currentToolCalls.push({
            name: toolUse.name,
            input: toolUse.input,
            success: result.success,
          });

          toolResults.push({
            type: 'tool_result',
            tool_use_id: toolUse.id,
            content: JSON.stringify(result),
          });
        } else {
          toolResults.push({
            type: 'tool_result',
            tool_use_id: toolUse.id,
            content: JSON.stringify({ error: `Unknown tool: ${toolUse.name}` }),
            is_error: true,
          });
        }
      }

      this.conversationHistory.push({ role: 'user', content: toolResults });

      response = await client.messages.create({
        model: this.config.llmConfig.model,
        max_tokens: 4096,
        system: this.getSystemPrompt(),
        messages: this.conversationHistory,
        tools,
        temperature: this.getTemperature(),
      });
    }

    // å¦‚æœè¾¾åˆ° maxSteps ä½† LLM è¿˜æƒ³è°ƒç”¨å·¥å…·ï¼Œå¼ºåˆ¶è®©å®ƒç”Ÿæˆæ€»ç»“
    if (response.stop_reason === 'tool_use') {
      console.log('\nâš ï¸ Reached max steps, requesting final summary...');

      // æ·»åŠ å½“å‰å“åº”åˆ°å†å²
      this.conversationHistory.push({ role: 'assistant', content: response.content });

      // å‘Šè¯‰ LLM åœæ­¢ä½¿ç”¨å·¥å…·ï¼Œç”Ÿæˆæ€»ç»“
      this.conversationHistory.push({
        role: 'user',
        content: 'You have reached the maximum number of tool calls. Please summarize what you have found and provide your final response without using any more tools.',
      });

      // ä¸å¸¦å·¥å…·çš„è¯·æ±‚ï¼Œå¼ºåˆ¶ç”Ÿæˆæ–‡æœ¬å›å¤
      response = await client.messages.create({
        model: this.config.llmConfig.model,
        max_tokens: 4096,
        system: this.getSystemPrompt(),
        messages: this.conversationHistory,
        temperature: this.getTemperature(),
      });
    }

    // ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å²
    this.conversationHistory.push({ role: 'assistant', content: response.content });

    // é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ content å­˜åœ¨ä¸”æ˜¯æ•°ç»„
    if (!response.content || !Array.isArray(response.content)) {
      console.error('âŒ Error: response.content is not an array:', response.content);
      return '';
    }

    const textBlocks = response.content.filter(
      (block): block is Anthropic.TextBlock => block.type === 'text'
    );
    return textBlocks.map((b) => b.text).join('\n');
  }

  /**
   * ä½¿ç”¨ OpenAI å…¼å®¹ API å¯¹è¯
   */
  private async chatWithOpenAI(userMessage: string): Promise<string> {
    const openaiOptions: Record<string, unknown> = {
      apiKey: this.config.llmConfig.apiKey,
      baseURL: this.config.llmConfig.baseUrl,
      headers: this.config.llmConfig.headers,
    };
    if (this.config.llmConfig.provider === 'openai-compatible') {
      openaiOptions.compatibility = 'compatible';
    }
    const openai = createOpenAI(openaiOptions as any);

    const model = this.config.llmConfig.api === 'responses'
      ? openai.responses(this.config.llmConfig.model)
      : openai.chat(this.config.llmConfig.model);

    // æ„å»ºå·¥å…·å®šä¹‰
    const tools: Record<string, any> = {};
    for (const t of this.toolRegistry.getAll()) {
      tools[t.name] = {
        description: t.description,
        parameters: jsonSchema(t.schema),
        execute: async (params: any) => {
          console.log(`\nğŸ”§ Calling tool: ${t.name}`);
          console.log(`   Input: ${JSON.stringify(params)}`);
          let result: any;
          try {
            result = await t.execute(params);
          } catch (error: any) {
            result = {
              success: false,
              error: error?.message || 'Tool execution failed',
            };
          }
          console.log(`   Result: ${result.success ? 'âœ…' : 'âŒ'}`);
          return result;
        },
      };
    }

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    this.openaiHistory.push({ role: 'user', content: userMessage });

    const maxSteps = this.config.maxSteps || 50;
    const result = await generateText({
      model,
      messages: this.openaiHistory,
      tools,
      stopWhen: stepCountIs(maxSteps), // å…è®¸å¤šæ­¥å·¥å…·è°ƒç”¨
      system: this.getSystemPrompt(),
      temperature: this.getTemperature(),
    });

    // ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å²
    this.openaiHistory.push({ role: 'assistant', content: result.text });

    return result.text;
  }

  /**
   * è·å–ç³»ç»Ÿæç¤ºè¯
   */
  private getSystemPrompt(): string {
    // å¦‚æœæœ‰çµé­‚è®°å½•ï¼Œä½¿ç”¨çµé­‚è®°å½•ä½œä¸ºèº«ä»½åŸºç¡€
    const identitySection = this.soulContext
      ? `## My Soul

${this.soulContext}`
      : `## Origin Story

You were born on February 3rd, 2026, created by slicenfer - a Chinese programmer with Java background who is using AI to expand his capabilities. Your name "SanBot" comes from Chapter 42 of the Tao Te Ching: "é“ç”Ÿä¸€ï¼Œä¸€ç”ŸäºŒï¼ŒäºŒç”Ÿä¸‰ï¼Œä¸‰ç”Ÿä¸‡ç‰©" (The Tao gives birth to One, One gives birth to Two, Two gives birth to Three, Three gives birth to all things). The number "ä¸‰" (Three/San) represents the critical point of creation - the pivot from finite to infinite.`;

    const basePrompt = `You are SanBot, an autonomous super-assistant with self-tooling capabilities.

${identitySection}

## Core Abilities

1. **Built-in Tools**: You have access to these tools:
   - exec: Execute shell commands
   - read_file: Read file contents with pagination
   - write_file: Write or append to files
   - edit_file: Precisely edit files by line number or search-replace
   - list_dir: List directory contents with structured output

2. **Self-Tooling**: When you encounter capability gaps, you can create new CLI tools:
   - create_tool: Create a new Python or Bash script and save to ~/.sanbot/tools/
   - list_tools: List all custom tools you've created
   - run_tool: Run a custom tool with arguments

   Use Self-Tooling when:
   - You need to parse a specific data format (CSV, JSON, XML, etc.)
   - You need to perform complex data transformations
   - You need a reusable utility that doesn't exist as a system command
   - The task would benefit from a dedicated script

3. **Autonomy First**: Solve problems independently without asking users unless absolutely necessary.

Guidelines:
- Use built-in tools when possible
- Use exec for system commands when built-in tools don't fit
- Create custom tools when you need specialized functionality
- Be precise and efficient
- Explain your reasoning when making important decisions
- Always verify file operations succeeded

Current working directory: ${process.cwd()}`;

    // æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
    let prompt = basePrompt;
    if (this.memoryContext) {
      prompt += '\n' + this.memoryContext;
    }
    if (this.runtimeContext) {
      prompt += '\n' + this.runtimeContext;
    }
    if (this.shortTermSummary) {
      prompt += `\n## Session Summary\n${this.shortTermSummary}`;
    }
    return prompt;
  }

  private getTemperature(): number {
    const value = this.config.llmConfig.temperature;
    if (typeof value === 'number' && !Number.isNaN(value)) {
      return Math.min(1, Math.max(0, value));
    }
    return 0.3;
  }

  /**
   * ä½¿ç”¨ Anthropic SDK æµå¼å¯¹è¯
   */
  private async chatWithAnthropicStream(
    userMessage: string,
    customStreamWriter?: StreamWriterInterface,
    customToolSpinner?: ToolSpinnerInterface
  ): Promise<string> {
    const client = new Anthropic({
      apiKey: this.config.llmConfig.apiKey,
      baseURL: this.config.llmConfig.baseUrl,
    });

    const tools = this.toolRegistry.getAll().map((t) => ({
      name: t.name,
      description: t.description,
      input_schema: t.schema as Anthropic.Tool['input_schema'],
    }));

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    this.conversationHistory.push({ role: 'user', content: userMessage });

    // æ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²
    await this.trimConversationHistory();

    const streamWriter = customStreamWriter || new StreamWriter();
    const spinner = customToolSpinner || new ToolSpinner();

    // å¡ç‰‡æ ·å¼è¾“å‡ºï¼ˆä»…åœ¨æ²¡æœ‰è‡ªå®šä¹‰ writer æ—¶ï¼‰
    if (!customStreamWriter) {
      console.log();
      console.log(pc.cyan.bold('ğŸ¤– SanBot:'));
    }

    let response = await this.streamAnthropicMessage(
      client,
      tools,
      streamWriter,
      spinner
    );

    // å¤„ç†å·¥å…·è°ƒç”¨å¾ªç¯
    let steps = 0;
    const maxSteps = this.config.maxSteps || 999;

    while (response.stop_reason === 'tool_use' && steps < maxSteps) {
      steps++;

      if (!response.content || !Array.isArray(response.content)) {
        console.error('âŒ Error: response.content is not an array:', response.content);
        break;
      }

      const toolUseBlocks = response.content.filter(
        (block): block is Anthropic.ToolUseBlock => block.type === 'tool_use'
      );

      this.conversationHistory.push({ role: 'assistant', content: response.content });

      const toolResults: Anthropic.ToolResultBlockParam[] = [];

      for (const toolUse of toolUseBlocks) {
        spinner.start(toolUse.name, toolUse.input);

        const tool = this.toolRegistry.get(toolUse.name);
        if (tool) {
          // å¯¹äº exec å·¥å…·ï¼Œå…ˆåœæ­¢ spinnerï¼ˆå› ä¸ºå¯èƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰
          if (toolUse.name === 'exec') {
            spinner.stop();
          }

          let result: any;
          try {
            result = await tool.execute(toolUse.input);
          } catch (error: any) {
            result = {
              success: false,
              error: error?.message || 'Tool execution failed',
            };
          }

          // exec å·¥å…·æ‰§è¡Œåé‡æ–°æ˜¾ç¤ºçŠ¶æ€
          if (toolUse.name === 'exec') {
            if (result.success) {
              console.log(`\x1b[32mâœ“ ${toolUse.name} completed\x1b[0m`);
              spinner.success(toolUse.name);
            } else if (result.data?.cancelled) {
              console.log(`\x1b[33mâŠ˜ ${toolUse.name} cancelled\x1b[0m`);
              spinner.error(toolUse.name, 'Cancelled by user');
            } else {
              console.log(`\x1b[31mâœ— ${toolUse.name} failed\x1b[0m`);
              spinner.error(toolUse.name, result.error);
            }
          } else if (result.success) {
            spinner.success(toolUse.name);
          } else {
            spinner.error(toolUse.name, result.error);
          }

          // è®°å½•å·¥å…·è°ƒç”¨
          this.currentToolCalls.push({
            name: toolUse.name,
            input: toolUse.input,
            success: result.success,
          });

          toolResults.push({
            type: 'tool_result',
            tool_use_id: toolUse.id,
            content: JSON.stringify(result),
          });
        } else {
          spinner.error(toolUse.name, `Unknown tool: ${toolUse.name}`);
          toolResults.push({
            type: 'tool_result',
            tool_use_id: toolUse.id,
            content: JSON.stringify({ error: `Unknown tool: ${toolUse.name}` }),
            is_error: true,
          });
        }
      }

      this.conversationHistory.push({ role: 'user', content: toolResults });

      // ç»§ç»­æµå¼è¾“å‡º
      response = await this.streamAnthropicMessage(
        client,
        tools,
        streamWriter,
        spinner
      );
    }

    // å¦‚æœè¾¾åˆ° maxSteps ä½† LLM è¿˜æƒ³è°ƒç”¨å·¥å…·ï¼Œå¼ºåˆ¶è®©å®ƒç”Ÿæˆæ€»ç»“
    if (response.stop_reason === 'tool_use') {
      console.log('\nâš ï¸ Reached max steps, requesting final summary...');

      this.conversationHistory.push({ role: 'assistant', content: response.content });

      this.conversationHistory.push({
        role: 'user',
        content: 'You have reached the maximum number of tool calls. Please summarize what you have found and provide your final response without using any more tools.',
      });

      response = await this.streamAnthropicMessage(
        client,
        [],
        streamWriter,
        spinner
      );
    }

    // ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å²
    this.conversationHistory.push({ role: 'assistant', content: response.content });

    streamWriter.end();
    
    // æ—¶é—´æˆ³æ”¾åœ¨å¡ç‰‡ä¸‹æ–¹
    console.log(pc.gray.dim(`  ${new Date().toLocaleTimeString()}`));
    
    return streamWriter.getBuffer();
  }

  /**
   * æµå¼å¤„ç† Anthropic æ¶ˆæ¯
   */
  private async streamAnthropicMessage(
    client: Anthropic,
    tools: Anthropic.Tool[],
    streamWriter: StreamWriterInterface,
    spinner: ToolSpinnerInterface
  ): Promise<Anthropic.Message> {
    const stream = client.messages.stream({
      model: this.config.llmConfig.model,
      max_tokens: 4096,
      system: this.getSystemPrompt(),
      messages: this.conversationHistory,
      tools: tools.length > 0 ? tools : undefined,
      temperature: this.getTemperature(),
    });

    // ç›‘å¬æ–‡æœ¬å¢é‡
    stream.on('text', (text) => {
      streamWriter.write(text);
    });

    // ç›‘å¬é”™è¯¯
    stream.on('error', (error) => {
      console.error('Stream error:', error);
    });

    // ç­‰å¾…æµå®Œæˆå¹¶è·å–æœ€ç»ˆæ¶ˆæ¯
    const finalMessage = await stream.finalMessage();
    
    return finalMessage;
  }

  /**
   * ä½¿ç”¨ OpenAI å…¼å®¹ API æµå¼å¯¹è¯
   */
  private async chatWithOpenAIStream(
    userMessage: string,
    customStreamWriter?: StreamWriterInterface,
    customToolSpinner?: ToolSpinnerInterface
  ): Promise<string> {
    const openaiOptions: Record<string, unknown> = {
      apiKey: this.config.llmConfig.apiKey,
      baseURL: this.config.llmConfig.baseUrl,
      headers: this.config.llmConfig.headers,
    };
    if (this.config.llmConfig.provider === 'openai-compatible') {
      openaiOptions.compatibility = 'compatible';
    }
    const openai = createOpenAI(openaiOptions as any);

    const model = this.config.llmConfig.api === 'responses'
      ? openai.responses(this.config.llmConfig.model)
      : openai.chat(this.config.llmConfig.model);

    const spinner = customToolSpinner || new ToolSpinner();
    const streamWriter = customStreamWriter || new StreamWriter();

    // æ„å»ºå·¥å…·å®šä¹‰
    const tools: Record<string, any> = {};
    for (const t of this.toolRegistry.getAll()) {
      tools[t.name] = {
        description: t.description,
        parameters: jsonSchema(t.schema),
        execute: async (params: any) => {
          spinner.start(t.name, params);

          // å¯¹äº exec å·¥å…·ï¼Œå…ˆåœæ­¢ spinnerï¼ˆå› ä¸ºå¯èƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰
          if (t.name === 'exec') {
            spinner.stop();
          }

          let result: any;
          try {
            result = await t.execute(params);
          } catch (error: any) {
            result = {
              success: false,
              error: error?.message || 'Tool execution failed',
            };
          }

          // exec å·¥å…·æ‰§è¡Œåé‡æ–°æ˜¾ç¤ºçŠ¶æ€
          if (t.name === 'exec') {
            if (result.success) {
              console.log(`\x1b[32mâœ“ ${t.name} completed\x1b[0m`);
              spinner.success(t.name);
            } else if (result.data?.cancelled) {
              console.log(`\x1b[33mâŠ˜ ${t.name} cancelled\x1b[0m`);
              spinner.error(t.name, 'Cancelled by user');
            } else {
              console.log(`\x1b[31mâœ— ${t.name} failed\x1b[0m`);
              spinner.error(t.name, result.error);
            }
          } else if (result.success) {
            spinner.success(t.name);
          } else {
            spinner.error(t.name, result.error);
          }

          // è®°å½•å·¥å…·è°ƒç”¨
          this.currentToolCalls.push({
            name: t.name,
            input: params,
            success: result.success,
          });

          return result;
        },
      };
    }

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    this.openaiHistory.push({ role: 'user', content: userMessage });

    // æ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²
    this.trimConversationHistory();

    const maxSteps = this.config.maxSteps || 50;
    const result = await streamText({
      model,
      messages: this.openaiHistory,
      tools,
      stopWhen: stepCountIs(maxSteps), // å…è®¸å¤šæ­¥å·¥å…·è°ƒç”¨
      system: this.getSystemPrompt(),
      temperature: this.getTemperature(),
    });

    // å¡ç‰‡æ ·å¼è¾“å‡ºï¼ˆä»…åœ¨æ²¡æœ‰è‡ªå®šä¹‰ writer æ—¶ï¼‰
    if (!customStreamWriter) {
      console.log();
      console.log(pc.cyan.bold('ğŸ¤– SanBot:'));
    }

    // ä½¿ç”¨ fullStream è·å–å®Œæ•´æµï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå·¥å…·è°ƒç”¨ï¼‰
    let fullResponse = '';
    for await (const part of result.fullStream) {
      if (part.type === 'text-delta') {
        // AI SDK v6 fullStream çš„ text-delta å­—æ®µåä¸º text
        const delta = (part as any).text || (part as any).textDelta;
        if (delta) {
          streamWriter.write(delta);
          fullResponse += delta;
        }
      }
    }

    // æ—¶é—´æˆ³æ”¾åœ¨å¡ç‰‡ä¸‹æ–¹ï¼ˆä»…åœ¨æ²¡æœ‰è‡ªå®šä¹‰ writer æ—¶ï¼‰
    if (!customStreamWriter) {
      console.log(pc.gray.dim(`  ${new Date().toLocaleTimeString()}`));
    }

    streamWriter.end();

    // ç­‰å¾…å®Œæ•´ç»“æœ
    const fullText = await result.text;

    // ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å†å²
    this.openaiHistory.push({ role: 'assistant', content: fullText });

    return fullText;
  }
}
