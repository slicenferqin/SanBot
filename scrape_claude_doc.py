#!/usr/bin/env python3
import sys
import requests
from bs4 import BeautifulSoup
import json

# é€šè¿‡ä»£ç†æŠ“å– Claude æ–‡æ¡£
def scrape_claude_docs():
    proxy_url = "http://127.0.0.1:7897"
    proxies = {'http': proxy_url, 'https': proxy_url}
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    # Claude Agent SDK æ–‡æ¡£é¡µé¢
    urls = [
        'https://platform.claude.com/docs/en/agent-sdk/sessions',
        'https://platform.claude.com/docs/en/agent-sdk/getting-started',
        'https://platform.claude.com/docs/en/agent-sdk/overview',
    ]
    
    results = []
    
    for url in urls:
        try:
            print(f"ğŸ“¥ Fetching: {url}")
            resp = requests.get(url, proxies=proxies, headers=headers, timeout=30)
            print(f"âœ… Status: {resp.status_code}")
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for elem in soup(['script', 'style', 'nav', 'footer', 'header']):
                elem.decompose()
            
            # è·å–æ ‡é¢˜
            title = soup.title.string if soup.title else "No title"
            
            # è·å–ä¸»è¦å†…å®¹
            main = soup.find('main') or soup.find('article') or soup.find(['div'], class_=lambda x: x and ('content' in x.lower() or 'doc' in x.lower()))
            
            if not main:
                main = soup.find('body')
            
            if main:
                text = main.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            results.append({
                'url': url,
                'title': title.strip(),
                'content': text[:20000]
            })
            
        except Exception as e:
            print(f"âŒ Error fetching {url}: {e}")
    
    # ä¿å­˜ç»“æœ
    output_file = '/Users/slicenfer/Development/projects/self/SanBot/claude_docs.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Saved to: {output_file}")
    print(f"ğŸ“Š Total pages: {len(results)}")
    
    return results

if __name__ == '__main__':
    scrape_claude_docs()
