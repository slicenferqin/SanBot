#!/usr/bin/env python3
"""
æŠ–éŸ³æ•°æ®æŠ“å–å·¥å…· - ä½¿ç”¨ Playwright è‡ªåŠ¨åŒ–æµè§ˆå™¨
æ”¯æŒï¼šè·å–è´¦å·ä¿¡æ¯ã€ç²‰ä¸æ•°ã€ä½œå“æ•°ã€æœ€è¿‘ä½œå“çš„ç‚¹èµå’Œæ’­æ”¾é‡
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
from datetime import datetime


class DouyinScraper:
    def __init__(self, headless=False):
        self.headless = headless
        self.base_url = "https://www.douyin.com"
        
    async def scrape_user_info(self, username):
        """
        æŠ“å–æŠ–éŸ³ç”¨æˆ·ä¿¡æ¯
        :param username: ç”¨æˆ·åæˆ–æœç´¢å…³é”®è¯
        :return: ç”¨æˆ·ä¿¡æ¯å’Œæœ€è¿‘ä½œå“æ•°æ®
        """
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = await p.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-setuid-sandbox'
                ]
            )
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè®¾ç½®ç”¨æˆ·ä»£ç†
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            try:
                print(f"ğŸ” æ­£åœ¨æœç´¢ç”¨æˆ·: {username}")
                
                # è®¿é—®æŠ–éŸ³æœç´¢é¡µé¢
                search_url = f"{self.base_url}/search/{username}?type=user"
                print(f"ğŸ“‹ è®¿é—®URL: {search_url}")
                
                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(5)
                
                # æˆªå›¾è°ƒè¯•
                await page.screenshot(path="/tmp/douyin_search.png")
                print("ğŸ“¸ æœç´¢é¡µé¢å·²æˆªå›¾åˆ° /tmp/douyin_search.png")
                
                # å°è¯•æ‰¾åˆ°ç”¨æˆ·é“¾æ¥
                print("ğŸ“„ æ­£åœ¨æŸ¥æ‰¾ç”¨æˆ·ä¸»é¡µ...")
                
                user_page_url = None
                
                # æ–¹æ³•1: å°è¯•ä»æœç´¢ç»“æœä¸­æ‰¾åˆ°ç”¨æˆ·é“¾æ¥
                try:
                    # ç­‰å¾…æœç´¢ç»“æœä¸­çš„ç”¨æˆ·é“¾æ¥
                    await page.wait_for_selector('a[href*="/user/"]', timeout=15000)
                    
                    # è·å–æ‰€æœ‰ç”¨æˆ·é“¾æ¥
                    user_elements = await page.query_selector_all('a[href*="/user/"]')
                    print(f"ğŸ”— æ‰¾åˆ° {len(user_elements)} ä¸ªç”¨æˆ·é“¾æ¥")
                    
                    if user_elements:
                        for i, elem in enumerate(user_elements[:5]):  # åªçœ‹å‰5ä¸ª
                            href = await elem.get_attribute('href')
                            text = await elem.inner_text()
                            print(f"  [{i+1}] {text[:50]} -> {href}")
                        
                        # è·å–ç¬¬ä¸€ä¸ªç”¨æˆ·é“¾æ¥
                        href = await user_elements[0].get_attribute('href')
                        if href:
                            # å¤„ç†ç›¸å¯¹URLå’Œç»å¯¹URL
                            if href.startswith('http'):
                                user_page_url = href
                            elif href.startswith('/'):
                                user_page_url = f"{self.base_url}{href}"
                            else:
                                user_page_url = f"{self.base_url}/{href}"
                            print(f"âœ… æ‰¾åˆ°ç”¨æˆ·é¡µé¢: {user_page_url}")
                except Exception as e:
                    print(f"âš ï¸  æœç´¢æ–¹æ³•å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                if not user_page_url:
                    print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·é¡µé¢ï¼Œè¯·æ£€æŸ¥æœç´¢ç»“æœ")
                    return None
                
                # è®¿é—®ç”¨æˆ·ä¸»é¡µ
                print(f"ğŸš¶ æ­£åœ¨è®¿é—®ç”¨æˆ·ä¸»é¡µ...")
                print(f"ğŸ“‹ è®¿é—®URL: {user_page_url}")
                
                await page.goto(user_page_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
                
                # æˆªå›¾ç”¨æˆ·ä¸»é¡µ
                await page.screenshot(path="/tmp/douyin_user_page.png", full_page=True)
                print("ğŸ“¸ ç”¨æˆ·ä¸»é¡µå·²æˆªå›¾åˆ° /tmp/douyin_user_page.png")
                
                # è·å–é¡µé¢å†…å®¹ï¼ˆå°è¯•ä»é¡µé¢æ•°æ®ä¸­æå–ï¼‰
                page_content = await page.content()
                
                # å°è¯•ä»é¡µé¢æ•°æ®ä¸­æå–
                user_data = await self._extract_data_from_page(page)
                
                if user_data:
                    return user_data
                else:
                    return {
                        "error": "æ— æ³•è‡ªåŠ¨æå–æ•°æ®",
                        "screenshots": ["/tmp/douyin_search.png", "/tmp/douyin_user_page.png"],
                        "url": user_page_url,
                        "note": "è¯·æŸ¥çœ‹æˆªå›¾æˆ–æ‰‹åŠ¨æ£€æŸ¥é¡µé¢"
                    }
                
            except Exception as e:
                print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                return None
                
            finally:
                await browser.close()
    
    async def _extract_data_from_page(self, page):
        """
        ä»é¡µé¢ä¸­æå–æ•°æ®
        """
        data = {
            "timestamp": datetime.now().isoformat(),
            "user_info": {},
            "recent_videos": []
        }
        
        try:
            # æ–¹æ³•1: å°è¯•ä»é¡µé¢ script æ ‡ç­¾ä¸­æå– JSON æ•°æ®
            script_contents = await page.evaluate('''() => {
                const scripts = Array.from(document.querySelectorAll('script'));
                return scripts
                    .filter(s => s.textContent.includes('SSR_HYDRATED_DATA') || 
                              s.textContent.includes('__RENDER_DATA__'))
                    .map(s => s.textContent);
            }''')
            
            if script_contents and len(script_contents) > 0:
                print(f"âœ… æ‰¾åˆ° {len(script_contents)} ä¸ªæ•°æ®è„šæœ¬")
                
                # è§£æ JSON æ•°æ®
                for idx, script in enumerate(script_contents):
                    # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…
                    patterns = [
                        r'window\.__RENDER_DATA__\s*=\s*({.*?});',
                        r'window\._SSR_HYDRATED_DATA\s*=\s*({.*?});',
                        r'SSR_HYDRATED_DATA"\s*:\s*({.*?})',
                        r'__RENDER_DATA__\s*=\s*({.*?});'
                    ]
                    
                    for pattern in patterns:
                        match = re.search(pattern, script, re.DOTALL)
                        if match:
                            try:
                                json_str = match.group(1)
                                # æ¸…ç†JSONå­—ç¬¦ä¸²
                                json_str = json_str.replace('undefined', 'null')
                                parsed_data = json.loads(json_str)
                                
                                print(f"âœ… æˆåŠŸæå–æ•°æ® (æ¨¡å¼ {pattern[:30]}...)!")
                                data["raw_data"] = parsed_data
                                
                                # å°è¯•è§£æå…·ä½“æ•°æ®
                                await self._parse_user_data(parsed_data, data)
                                
                                return data
                                
                            except (json.JSONDecodeError, Exception) as e:
                                print(f"âš ï¸  JSON è§£æå¤±è´¥: {e}")
                                continue
            
            # æ–¹æ³•2: ä½¿ç”¨é¡µé¢é€‰æ‹©å™¨æå–å¯è§æ•°æ®
            print("ğŸ” å°è¯•ä»å¯è§å…ƒç´ æå–æ•°æ®...")
            
            visible_data = await page.evaluate('''() => {
                const result = {
                    page_title: document.title,
                    body_text: document.body.textContent.substring(0, 2000)
                };
                
                // å°è¯•æ‰¾åˆ°ç²‰ä¸æ•°ã€å…³æ³¨æ•°ç­‰
                const countSelectors = [
                    '[data-e2e="user-post-count"]',
                    '[data-e2e="user-following-count"]',
                    '[data-e2e="user-follower-count"]',
                    '[data-e2e="user-like-count"]',
                    '.user-info',
                    '.stats-info'
                ];
                
                countSelectors.forEach(sel => {
                    const elements = document.querySelectorAll(sel);
                    if (elements.length > 0) {
                        result[sel] = Array.from(elements).map(el => el.textContent);
                    }
                });
                
                return result;
            }''')
            
            print(f"ğŸ“ å¯è§æ•°æ®: {json.dumps(visible_data, indent=2, ensure_ascii=False)[:500]}")
            data["visible_data"] = visible_data
            
            return data
            
        except Exception as e:
            print(f"âš ï¸  æ•°æ®æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            data["error"] = str(e)
            return data
    
    async def _parse_user_data(self, raw_data, data):
        """
        è§£æç”¨æˆ·æ•°æ®
        æ³¨æ„ï¼šæŠ–éŸ³çš„æ•°æ®ç»“æ„å¯èƒ½ä¼šç»å¸¸å˜åŒ–ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        """
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ•°æ®ç»“æ„æ¥è§£æ
            # æ‰“å°æ•°æ®ç»“æ„ä»¥ä¾¿è°ƒè¯•
            print("ğŸ” è§£æç”¨æˆ·æ•°æ®...")
            
            # ä¿å­˜å®Œæ•´çš„æ•°æ®ç»“æ„ä¾›åˆ†æ
            data_structure = json.dumps(raw_data, indent=2, ensure_ascii=False)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ä¾›åˆ†æ
            structure_file = f"/tmp/douyin_data_structure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(structure_file, 'w', encoding='utf-8') as f:
                f.write(data_structure[:100000])  # é™åˆ¶å¤§å°
            print(f"ğŸ’¾ æ•°æ®ç»“æ„å·²ä¿å­˜åˆ°: {structure_file}")
            
        except Exception as e:
            print(f"âš ï¸  è§£æå¤±è´¥: {e}")


async def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆheadless=False å¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œï¼‰
    scraper = DouyinScraper(headless=False)
    
    # æœç´¢ç›®æ ‡
    username = "è´¾ä¹ƒäº®"
    
    print("=" * 60)
    print("ğŸ¬ æŠ–éŸ³æ•°æ®æŠ“å–å·¥å…· (Playwrightç‰ˆæœ¬)")
    print("=" * 60)
    print(f"ğŸ“Œ ç›®æ ‡ç”¨æˆ·: {username}")
    print(f"ğŸ“… æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # å¼€å§‹æŠ“å–
    result = await scraper.scrape_user_info(username)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æŠ“å–ç»“æœ")
    print("=" * 60)
    
    if result:
        print(json.dumps(result, indent=2, ensure_ascii=False)[:2000])
        if len(json.dumps(result)) > 2000:
            print("\n... (ç»“æœè¿‡é•¿ï¼Œå·²æˆªæ–­)")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = f"/tmp/douyin_{username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print("âŒ æŠ“å–å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(main())
