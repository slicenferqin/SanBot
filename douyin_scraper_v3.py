#!/usr/bin/env python3
"""
æŠ–éŸ³æ•°æ®æŠ“å–å·¥å…· V3 - ç”¨æˆ·è´¦å·ä¸“ç”¨ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹æœç´¢ç”¨æˆ·è´¦å·å’Œæ•°æ®æå–
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright
from datetime import datetime


class DouyinUserScraper:
    def __init__(self, headless=False):
        self.headless = headless
        self.base_url = "https://www.douyin.com"
        
    async def search_user_account(self, username):
        """
        æœç´¢ç”¨æˆ·è´¦å·
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            # ä½¿ç”¨æ¡Œé¢æµè§ˆå™¨ç¯å¢ƒ
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            try:
                print(f"ğŸ” æœç´¢ç”¨æˆ·: {username}")
                
                # æ–¹æ³•1: å°è¯•ä¸åŒçš„æœç´¢URLæ ¼å¼
                search_urls = [
                    f"{self.base_url}/search/{username}?type=user",
                    f"{self.base_url}/search/{username}",
                    f"https://www.douyin.com/search/user?keyword={username}",
                ]
                
                user_page_url = None
                
                for search_url in search_urls:
                    try:
                        print(f"ğŸ“‹ å°è¯•æœç´¢URL: {search_url}")
                        await page.goto(search_url, wait_until='domcontentloaded', timeout=20000)
                        await asyncio.sleep(5)
                        
                        # æˆªå›¾
                        screenshot_num = search_urls.index(search_url) + 1
                        await page.screenshot(path=f"/tmp/douyin_search_{screenshot_num}.png", full_page=True)
                        
                        # å°è¯•æ‰¾åˆ°ç”¨æˆ·å¡ç‰‡
                        user_found = await self._find_user_in_results(page, username)
                        if user_found:
                            user_page_url = user_found
                            print(f"âœ… æ‰¾åˆ°ç”¨æˆ·ä¸»é¡µ: {user_page_url}")
                            break
                    except Exception as e:
                        print(f"âš ï¸  æœç´¢å¤±è´¥: {e}")
                        continue
                
                if not user_page_url:
                    print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·ä¸»é¡µï¼Œä¿å­˜å½“å‰é¡µé¢ä¾›åˆ†æ")
                    await page.screenshot(path="/tmp/douyin_final_page.png", full_page=True)
                    
                    # ä¿å­˜é¡µé¢HTML
                    content = await page.content()
                    html_file = f"/tmp/douyin_page_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                    with open(html_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                    print(f"ğŸ’¾ é¡µé¢HTMLå·²ä¿å­˜: {html_file}")
                    
                    return {
                        "status": "not_found",
                        "searched_username": username,
                        "screenshot": "/tmp/douyin_final_page.png",
                        "html_file": html_file,
                        "note": "æœªæ‰¾åˆ°ç”¨æˆ·è´¦å·ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æˆªå›¾æˆ–HTMLæ–‡ä»¶"
                    }
                
                # è®¿é—®ç”¨æˆ·ä¸»é¡µ
                print(f"ğŸš¶ è®¿é—®ç”¨æˆ·ä¸»é¡µ: {user_page_url}")
                await page.goto(user_page_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(5)
                
                # æˆªå›¾ç”¨æˆ·ä¸»é¡µ
                user_screenshot = f"/tmp/douyin_user_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                await page.screenshot(path=user_screenshot, full_page=True)
                print(f"ğŸ“¸ ç”¨æˆ·ä¸»é¡µæˆªå›¾: {user_screenshot}")
                
                # æå–æ•°æ®
                user_data = await self._extract_user_data(page)
                user_data['user_page_url'] = user_page_url
                user_data['screenshot'] = user_screenshot
                
                return user_data
                
            except Exception as e:
                print(f"âŒ æŠ“å–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return None
            finally:
                await browser.close()
    
    async def _find_user_in_results(self, page, username):
        """
        ä»æœç´¢ç»“æœä¸­æ‰¾åˆ°ç”¨æˆ·é“¾æ¥
        """
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(2)
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« /user/ çš„é“¾æ¥
            user_links = await page.evaluate('''(username) => {
                const links = [];
                
                // æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                const allLinks = document.querySelectorAll('a');
                
                allLinks.forEach(link => {
                    const href = link.getAttribute('href');
                    if (!href) return;
                    
                    // æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·é“¾æ¥
                    if (href.includes('/user/') || href.includes('sec_user_id') || href.includes('user_id')) {
                        const text = link.textContent.trim();
                        // æ£€æŸ¥æ˜¯å¦åŒ…å«ç”¨æˆ·å
                        if (text.includes(username) || text.length < 100) {
                            links.push({
                                href: href,
                                text: text.substring(0, 100)
                            });
                        }
                    }
                });
                
                // å»é‡
                const seen = new Set();
                return links.filter(link => {
                    const key = link.href;
                    if (seen.has(key)) return false;
                    seen.add(key);
                    return true;
                });
            }''', username)
            
            if user_links:
                print(f"\nğŸ“‹ æ‰¾åˆ° {len(user_links)} ä¸ªå¯èƒ½çš„ç”¨æˆ·é“¾æ¥:")
                for i, link in enumerate(user_links[:10]):
                    print(f"  [{i+1}] {link['text'][:50]}")
                    print(f"      {link['href'][:100]}")
                
                # è¿”å›ç¬¬ä¸€ä¸ªç”¨æˆ·é“¾æ¥ï¼ˆé€šå¸¸æ˜¯æœ€ç›¸å…³çš„ï¼‰
                first_link = user_links[0]['href']
                
                # å¤„ç†ç›¸å¯¹URL
                if first_link.startswith('/'):
                    return f"{self.base_url}{first_link}"
                elif first_link.startswith('http'):
                    return first_link
                else:
                    return f"{self.base_url}/{first_link}"
            
            return None
            
        except Exception as e:
            print(f"âš ï¸  æŸ¥æ‰¾ç”¨æˆ·é“¾æ¥å¤±è´¥: {e}")
            return None
    
    async def _extract_user_data(self, page):
        """
        ä»ç”¨æˆ·ä¸»é¡µæå–æ•°æ®
        """
        data = {
            "timestamp": datetime.now().isoformat(),
            "user_info": {},
            "stats": {},
            "videos": []
        }
        
        try:
            print("ğŸ” æå–ç”¨æˆ·æ•°æ®...")
            
            # 1. è·å–é¡µé¢æºç 
            page_content = await page.content()
            
            # 2. å°è¯•ä» script æ ‡ç­¾æå–æ¸²æŸ“æ•°æ®
            script_data = await page.evaluate('''() => {
                const result = {
                    render_data: null,
                    ssr_data: null
                };
                
                // æŸ¥æ‰¾ __RENDER_DATA__
                const renderScript = Array.from(document.querySelectorAll('script')).find(s => 
                    s.textContent.includes('__RENDER_DATA__')
                );
                
                if (renderScript) {
                    const match = renderScript.textContent.match(/__RENDER_DATA__\s*=\s*({.+?});/);
                    if (match) {
                        try {
                            result.render_data = JSON.parse(match[1]);
                        } catch (e) {
                            result.render_data = 'parse_error';
                        }
                    }
                }
                
                // æŸ¥æ‰¾ _SSR_HYDRATED_DATA
                const ssrScript = Array.from(document.querySelectorAll('script')).find(s => 
                    s.textContent.includes('_SSR_HYDRATED_DATA')
                );
                
                if (ssrScript) {
                    const match = ssrScript.textContent.match(/_SSR_HYDRATED_DATA\s*=\s*({.+?});/);
                    if (match) {
                        try {
                            result.ssr_data = JSON.parse(match[1]);
                        } catch (e) {
                            result.ssr_data = 'parse_error';
                        }
                    }
                }
                
                return result;
            }''')
            
            if script_data['render_data']:
                print("âœ… æ‰¾åˆ° RENDER_DATA")
                data['render_data'] = script_data['render_data']
                # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥è§£æå…·ä½“çš„æ•°æ®ç»“æ„
                
            if script_data['ssr_data']:
                print("âœ… æ‰¾åˆ° SSR_HYDRATED_DATA")
                data['ssr_data'] = script_data['ssr_data']
            
            # 3. ä»å¯è§å…ƒç´ æå–æ•°æ®
            visible_stats = await page.evaluate('''() => {
                const result = {};
                
                // æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„å…ƒç´ ï¼ˆå¯èƒ½æ˜¯ç²‰ä¸æ•°ã€ä½œå“æ•°ç­‰ï¼‰
                const allElements = document.querySelectorAll('*');
                const numbersWithText = [];
                
                allElements.forEach(el => {
                    const text = el.textContent.trim();
                    // åŒ¹é…æ¨¡å¼ï¼šç²‰ä¸ã€å…³æ³¨ã€è·èµã€ä½œå“ + æ•°å­—
                    if (/^(ç²‰ä¸|å…³æ³¨|è·èµ|ä½œå“|ç‚¹èµ)/.test(text)) {
                        const parent = el.parentElement;
                        if (parent) {
                            const value = parent.textContent.trim();
                            numbersWithText.push(value);
                        }
                    }
                });
                
                result.stats_text = numbersWithText;
                
                // è·å–é¡µé¢æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
                result.page_title = document.title;
                result.url = window.location.href;
                
                return result;
            }''')
            
            data['visible_stats'] = visible_stats
            
            # 4. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»HTMLä¸­æå–æ•°æ®
            patterns = {
                'ç²‰ä¸': r'[ç²‰ä¸]?æ•°[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒç™¾ä¸‡]?)',
                'å…³æ³¨': r'[å…³æ³¨]?æ•°[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒç™¾ä¸‡]?)',
                'è·èµ': r'[è·èµ]?æ•°[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒç™¾ä¸‡]?)',
                'ä½œå“': r'[ä½œå“]?æ•°[ï¼š:\s]*(\d+(?:\.\d+)?[ä¸‡åƒç™¾ä¸‡]?)',
            }
            
            extracted = {}
            for key, pattern in patterns.items():
                matches = re.findall(pattern, page_content)
                if matches:
                    extracted[key] = matches[:3]
            
            data['regex_stats'] = extracted
            
            return data
            
        except Exception as e:
            print(f"âš ï¸  æ•°æ®æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            data['error'] = str(e)
            return data


async def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 70)
    print("ğŸ¬ æŠ–éŸ³ç”¨æˆ·æ•°æ®æŠ“å–å·¥å…· V3")
    print("=" * 70)
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    scraper = DouyinUserScraper(headless=False)
    
    # æœç´¢ç›®æ ‡
    username = "è´¾ä¹ƒäº®"
    
    print(f"\nğŸ¯ ç›®æ ‡ç”¨æˆ·: {username}\n")
    
    result = await scraper.search_user_account(username)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æŠ“å–ç»“æœ")
    print("=" * 70)
    
    if result:
        # æ‰“å°æ‘˜è¦
        print(f"\nçŠ¶æ€: {result.get('status', 'unknown')}")
        print(f"ç”¨æˆ·ä¸»é¡µ: {result.get('user_page_url', 'N/A')}")
        print(f"æˆªå›¾: {result.get('screenshot', 'N/A')}")
        
        if 'visible_stats' in result:
            print(f"\nå¯è§ç»Ÿè®¡æ•°æ®: {json.dumps(result['visible_stats'], indent=2, ensure_ascii=False)[:500]}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        output_file = f"/tmp/douyin_result_{username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {output_file}")
    
    print("\n" + "=" * 70)
    print("âœ… å®Œæˆ!")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())
